{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from minisom import MiniSom\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "INPUT_FILE = 'data_dec_2024.parquet'\n",
    "ARTIFACT_DIR = 'models2'\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_and_load(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df[df['DpiPolicy'] != 'Unknown']\n",
    "    \n",
    "    # Vectorized numeric conversion\n",
    "    numeric_cols = ['bytesFromClient', 'bytesFromServer', 'sessions_count', 'transationDuration']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace('\"', ''), errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def encode_to_bitmap(df, column_name, mapping_name):\n",
    "    \"\"\"Generic bitmap encoder to replace redundant loops.\"\"\"\n",
    "    # Define mappings based on your specific research requirements\n",
    "    vocab = {\n",
    "        'DpiPolicy': ['BLOCKALL', 'BlockNonProvSub', 'ClientIPWhitelist', 'ExpireBlock', 'F1200G50M', 'F3000G100M', 'F3000G200M', 'F4000G300M', 'F4000G400M', 'F4000G500M', 'F4000G550M', 'F4000G600M', 'F6000G1000M', 'NoCRBNBlock', 'SuspendBlock', 'ZeroRated'],\n",
    "        'contentType': ['Ads & Trackers', 'Browsing', 'Domain Name Service', 'Email', 'FTP', 'File Sharing', 'Gaming', 'Instant Messaging', 'Internet Privacy', 'Location Based Services', 'Music Streaming', 'Net Admin', 'One Click Hosting', 'Other', 'Other File Sharing', 'Social Media', 'Streaming', 'Trading', 'Unknown', 'Voice & Video Calls', 'Webmail'],\n",
    "        'IpProtocol': ['ESP', 'GRE', 'ICMP', 'TCP', 'UDP']\n",
    "    }\n",
    "    \n",
    "    mapping = {item: i for i, item in enumerate(vocab[column_name])}\n",
    "    joblib.dump(mapping, f'{ARTIFACT_DIR}/{mapping_name}.joblib')\n",
    "    \n",
    "    def calculate_bitsum(val):\n",
    "        if pd.isna(val): return 0\n",
    "        try:\n",
    "            items = val if isinstance(val, list) else ast.literal_eval(val) if (isinstance(val, str) and val.startswith('[')) else [val]\n",
    "            return sum(1 << mapping[i] for i in items if i in mapping)\n",
    "        except: return 0\n",
    "\n",
    "    return df[column_name].apply(calculate_bitsum)\n",
    "\n",
    "# === EXECUTION ===\n",
    "df = clean_and_load(INPUT_FILE)\n",
    "original_offers = df['DpiPolicy'].copy()\n",
    "\n",
    "# Encodings\n",
    "df['DpiPolicy'] = encode_to_bitmap(df, 'DpiPolicy', 'policy_to_bit')\n",
    "df['contentType'] = encode_to_bitmap(df, 'contentType', 'content_to_bit')\n",
    "df['IpProtocol'] = encode_to_bitmap(df, 'IpProtocol', 'proto_to_bit')\n",
    "df['app_count'] = df['appName'].apply(lambda x: len(x) if isinstance(x, list) else 1)\n",
    "\n",
    "# Imputation & Scaling\n",
    "medians = {col: max(df.loc[df[col] > 0, col].median(), 1) for col in ['bytesFromClient', 'bytesFromServer', 'transationDuration']}\n",
    "for col, val in medians.items():\n",
    "    df[col] = df[col].apply(lambda x: val if x <= 0 else x)\n",
    "joblib.dump(medians, f'{ARTIFACT_DIR}/medians.joblib')\n",
    "\n",
    "# Final Feature Selection\n",
    "features_df = df.select_dtypes(include=[np.number]).drop(columns=['SubscriberID'], errors='ignore')\n",
    "numeric_cols = features_df.columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features_df)\n",
    "joblib.dump(scaler, f'{ARTIFACT_DIR}/scaler.joblib')\n",
    "joblib.dump(numeric_cols, f'{ARTIFACT_DIR}/numeric_cols.joblib')\n",
    "\n",
    "# === SOM TRAINING ===\n",
    "rows, cols = 10, 10\n",
    "som = MiniSom(x=rows, y=cols, input_len=X.shape[1], sigma=0.8*sqrt(rows**2+cols**2), learning_rate=0.8)\n",
    "som.random_weights_init(X) # Standardized initialization is better than uniform [-0.3, 0.3]\n",
    "som.train_random(X, num_iteration=len(X))\n",
    "\n",
    "joblib.dump(som, f'{ARTIFACT_DIR}/som_model.joblib')\n",
    "np.save('som_weights1.npy', som.get_weights())\n",
    "\n",
    "# === CLUSTER MAPPING ===\n",
    "cluster_offers = {}\n",
    "# Find winners for all at once\n",
    "winners = np.array([som.winner(x) for x in X])\n",
    "cluster_ids = winners[:, 0] * cols + winners[:, 1]\n",
    "\n",
    "# Map back to original offers (Vectorized-style grouping)\n",
    "df['cluster_id'] = cluster_ids\n",
    "df['parsed_offers'] = original_offers.apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "for cid, group in df.groupby('cluster_id'):\n",
    "    all_offers = [o for sublist in group['parsed_offers'] for o in sublist]\n",
    "    excluded = {'F3000G200M', 'F3000G100M', 'F1200G50M'}\n",
    "    counts = Counter([o for o in all_offers if o not in excluded])\n",
    "    \n",
    "    # Logic: 20% threshold or top 2\n",
    "    threshold = len(group) * 0.2\n",
    "    top = [o for o, c in counts.items() if c >= threshold]\n",
    "    cluster_offers[int(cid)] = top if len(top) >= 2 else [o for o, _ in counts.most_common(2)]\n",
    "\n",
    "joblib.dump(cluster_offers, f'{ARTIFACT_DIR}/centroid_feature_map.joblib')\n",
    "print(f\"Pipeline complete. {len(cluster_offers)} clusters mapped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
