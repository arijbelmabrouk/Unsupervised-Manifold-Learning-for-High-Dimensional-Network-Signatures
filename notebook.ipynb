{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c848d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# URL that returns parquet data\n",
    "url = \"\"\n",
    "# Make the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Raise an error if the request failed\n",
    "response.raise_for_status()\n",
    "\n",
    "# Read the parquet content into a pandas DataFrame using pyarrow engine\n",
    "df = pd.read_parquet(io.BytesIO(response.content), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4567224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import ast\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from minisom import MiniSom\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create models2 directory if it doesn't exist\n",
    "os.makedirs('models2', exist_ok=True)\n",
    "\n",
    "# === SETUP ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data_dec_2024.parquet')\n",
    "df = df[df['DpiPolicy'] != 'Unknown']\n",
    "\n",
    "# Convert numeric columns if they're stored as strings\n",
    "numeric_columns = ['bytesFromClient', 'bytesFromServer', 'sessions_count', 'transationDuration']\n",
    "for col in numeric_columns:\n",
    "    # Check if column exists\n",
    "    if col in df.columns:\n",
    "        # Check if column is string type and contains quotes\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = df[col].str.replace('\"', '').astype(float)\n",
    "            except AttributeError:\n",
    "                # If it's already numeric, just convert to float\n",
    "                try:\n",
    "                    df[col] = df[col].astype(float)\n",
    "                except:\n",
    "                    print(f\"Warning: Could not convert column {col} to numeric\")\n",
    "\n",
    "result = df.groupby('SubscriberID').agg({\n",
    "    'DpiPolicy': lambda x: list(set(x)),\n",
    "    'appName': lambda x: list(set(x)),\n",
    "    'contentType': lambda x: list(set(x)),\n",
    "    'IpProtocol': lambda x: list(set(x)),\n",
    "    'bytesFromClient': 'sum',\n",
    "    'bytesFromServer': 'sum',\n",
    "    'sessions_count': 'sum',\n",
    "    'transationDuration': 'sum'\n",
    "})\n",
    "\n",
    "# Format the numeric columns to remove decimal points for integers\n",
    "result['bytesFromClient'] = result['bytesFromClient'].astype(int)\n",
    "result['bytesFromServer'] = result['bytesFromServer'].astype(int)\n",
    "result['sessions_count'] = result['sessions_count'].astype(int)\n",
    "result['transationDuration'] = result['transationDuration'].astype(int)\n",
    "\n",
    "# Keep track of original offers for each subscriber\n",
    "# For Parquet files, lists might already be proper Python lists, not strings\n",
    "original_offers = df['DpiPolicy'].copy()\n",
    "\n",
    "# === STEP 1: Encode DpiPolicy to bitmap ===\n",
    "policy_values = [\n",
    "    'BLOCKALL', 'BlockNonProvSub', 'ClientIPWhitelist', 'ExpireBlock',\n",
    "    'F1200G50M', 'F3000G100M', 'F3000G200M', 'F4000G300M', 'F4000G400M',\n",
    "    'F4000G500M', 'F4000G550M', 'F4000G600M', 'F6000G1000M',\n",
    "    'NoCRBNBlock', 'SuspendBlock', 'ZeroRated'\n",
    "]\n",
    "policy_to_bit = {policy: i for i, policy in enumerate(policy_values)}\n",
    "joblib.dump(policy_to_bit, 'models2/policy_to_bit.joblib')\n",
    "\n",
    "dpi_bitmap = []\n",
    "for val in df['DpiPolicy']:\n",
    "    if pd.isna(val):\n",
    "        dpi_bitmap.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            # Check if already a list (common in parquet) or needs evaluation (common in CSV)\n",
    "            if isinstance(val, list):\n",
    "                policies = val\n",
    "            elif isinstance(val, str):\n",
    "                # Try parsing if it's a string representation of a list\n",
    "                if val.startswith('[') and val.endswith(']'):\n",
    "                    try:\n",
    "                        policies = ast.literal_eval(val)\n",
    "                    except:\n",
    "                        policies = [val]  # If parsing fails, treat as single value\n",
    "                else:\n",
    "                    policies = [val]  # Single string value\n",
    "            else:\n",
    "                policies = [val]  # Single value of another type\n",
    "                \n",
    "            bitsum = sum(1 << policy_to_bit[p] for p in policies if p in policy_to_bit)\n",
    "            dpi_bitmap.append(bitsum)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DpiPolicy value: {val}, Error: {e}\")\n",
    "            dpi_bitmap.append(0)\n",
    "            \n",
    "df['DpiPolicy'] = dpi_bitmap\n",
    "\n",
    "# === STEP 2: Encode contentType to bitmap ===\n",
    "content_types = [\n",
    "    'Ads & Trackers', 'Browsing', 'Domain Name Service', 'Email', 'FTP',\n",
    "    'File Sharing', 'Gaming', 'Instant Messaging', 'Internet Privacy',\n",
    "    'Location Based Services', 'Music Streaming', 'Net Admin', 'One Click Hosting',\n",
    "    'Other', 'Other File Sharing', 'Social Media', 'Streaming', 'Trading',\n",
    "    'Unknown', 'Voice & Video Calls', 'Webmail'\n",
    "]\n",
    "content_to_bit = {c: i for i, c in enumerate(content_types)}\n",
    "joblib.dump(content_to_bit, 'models2/content_to_bit.joblib')\n",
    "\n",
    "content_bitmap = []\n",
    "for val in df['contentType']:\n",
    "    if pd.isna(val):\n",
    "        content_bitmap.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            # Similar approach as DpiPolicy\n",
    "            if isinstance(val, list):\n",
    "                contents = val\n",
    "            elif isinstance(val, str):\n",
    "                if val.startswith('[') and val.endswith(']'):\n",
    "                    try:\n",
    "                        contents = ast.literal_eval(val)\n",
    "                    except:\n",
    "                        contents = [val]\n",
    "                else:\n",
    "                    contents = [val]\n",
    "            else:\n",
    "                contents = [val]\n",
    "                \n",
    "            bitsum = sum(1 << content_to_bit[c] for c in contents if c in content_to_bit)\n",
    "            content_bitmap.append(bitsum)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing contentType value: {val}, Error: {e}\")\n",
    "            content_bitmap.append(0)\n",
    "            \n",
    "df['contentType'] = content_bitmap\n",
    "\n",
    "# === STEP 3: Encode IpProtocol to bitmap ===\n",
    "ip_protocols = ['ESP', 'GRE', 'ICMP', 'TCP', 'UDP']\n",
    "proto_to_bit = {p: i for i, p in enumerate(ip_protocols)}\n",
    "joblib.dump(proto_to_bit, 'models2/proto_to_bit.joblib')\n",
    "\n",
    "proto_bitmap = []\n",
    "for val in df['IpProtocol']:\n",
    "    if pd.isna(val):\n",
    "        proto_bitmap.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            # Similar approach as before\n",
    "            if isinstance(val, list):\n",
    "                protos = val\n",
    "            elif isinstance(val, str):\n",
    "                if val.startswith('[') and val.endswith(']'):\n",
    "                    try:\n",
    "                        protos = ast.literal_eval(val)\n",
    "                    except:\n",
    "                        protos = [val]\n",
    "                else:\n",
    "                    protos = [val]\n",
    "            else:\n",
    "                protos = [val]\n",
    "                \n",
    "            bitsum = sum(1 << proto_to_bit[p] for p in protos if p in proto_to_bit)\n",
    "            proto_bitmap.append(bitsum)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing IpProtocol value: {val}, Error: {e}\")\n",
    "            proto_bitmap.append(0)\n",
    "            \n",
    "df['IpProtocol'] = proto_bitmap\n",
    "\n",
    "# === STEP 4: Convert appName to app_count ===\n",
    "app_counts = []\n",
    "for val in df['appName']:\n",
    "    if pd.isna(val):\n",
    "        app_counts.append(0)\n",
    "    else:\n",
    "        try:\n",
    "            # Similar approach as before\n",
    "            if isinstance(val, list):\n",
    "                apps = val\n",
    "            elif isinstance(val, str):\n",
    "                if val.startswith('[') and val.endswith(']'):\n",
    "                    try:\n",
    "                        apps = ast.literal_eval(val)\n",
    "                    except:\n",
    "                        apps = [val]\n",
    "                else:\n",
    "                    apps = [val]\n",
    "            else:\n",
    "                apps = [val]\n",
    "                \n",
    "            app_counts.append(len(apps) if isinstance(apps, list) else 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing appName value: {val}, Error: {e}\")\n",
    "            app_counts.append(1)\n",
    "            \n",
    "df['app_count'] = app_counts\n",
    "df.drop(columns=['appName'], inplace=True)\n",
    "\n",
    "# Save columns affected\n",
    "joblib.dump(['appName'], 'models2/dropped_columns.joblib')\n",
    "joblib.dump(['app_count'], 'models2/added_columns.joblib')\n",
    "\n",
    "# === STEP 5: Check for and handle any remaining non-numeric columns ===\n",
    "# Identify object columns that need conversion\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"Found object columns that need conversion: {object_cols}\")\n",
    "    for col in object_cols:\n",
    "        print(f\"Converting column: {col}\")\n",
    "        # Try to convert string representations of lists to numeric values\n",
    "        try:\n",
    "            # For any remaining object columns, we'll drop them or convert as appropriate\n",
    "            # Here we'll convert to category codes as a default strategy\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col}: {e}\")\n",
    "            print(f\"Dropping column {col}\")\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# === STEP 6: Fix zeros/negatives ===\n",
    "medians = {}\n",
    "fix_cols = ['bytesFromClient', 'bytesFromServer', 'transationDuration']\n",
    "for col in fix_cols:\n",
    "    if col in df.columns:\n",
    "        median = df.loc[df[col] > 0, col].median()\n",
    "        median = median if not np.isnan(median) else 1\n",
    "        medians[col] = median\n",
    "        df[col] = df[col].apply(lambda x: median if pd.isna(x) or x <= 0 else x)\n",
    "joblib.dump(medians, 'models2/medians.joblib')\n",
    "\n",
    "# === STEP 7: Standardize numeric columns ===\n",
    "# Make sure all columns are numeric at this point\n",
    "print(\"Columns before selecting numeric types:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"Data types before selecting numeric types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert any non-numeric columns that should be numeric\n",
    "# This step helps with columns that might have been lists in parquet\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or pd.api.types.is_object_dtype(df[col]):\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col].fillna(df[col].median() if not df[col].isna().all() else 0, inplace=True)\n",
    "            print(f\"Converted column {col} to numeric\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not convert column {col} to numeric: {e}\")\n",
    "            print(f\"Dropping column {col}\")\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Now select only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "print(\"Final data types before scaling:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "numeric_cols = df.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[numeric_cols])\n",
    "df[numeric_cols] = scaler.transform(df[numeric_cols])\n",
    "joblib.dump(scaler, 'models2/scaler.joblib')\n",
    "joblib.dump(numeric_cols, 'models2/numeric_cols.joblib')\n",
    "\n",
    "# Print sample of processed data to verify\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df.head())\n",
    "\n",
    "# === NOW START SOM MODEL TRAINING ===\n",
    "# Drop SubscriberID if it exists (identifier column not useful for training)\n",
    "if 'SubscriberID' in df.columns:\n",
    "    df.drop(columns=['SubscriberID'], inplace=True)\n",
    "    print(\"Dropped 'SubscriberID' column before training SOM.\")\n",
    "\n",
    "X = df.values  # Use the processed and scaled data for SOM\n",
    "print(f\"Data shape for SOM: {X.shape}\")\n",
    "print(f\"Data type: {X.dtype}\")\n",
    "\n",
    "# Check for any non-numeric values in X\n",
    "if np.issubdtype(X.dtype, np.number) is False:\n",
    "    print(\"WARNING: Non-numeric values detected in the data\")\n",
    "    # Convert data to float64 and replace any non-numeric values with 0\n",
    "    X = np.nan_to_num(X.astype(float))\n",
    "\n",
    "# === SOM Setup ===\n",
    "num_rows, num_cols = 10, 10\n",
    "grid_diagonal = sqrt(num_rows**2 + num_cols**2)\n",
    "sigma = 0.8 * grid_diagonal\n",
    "learning_rate = 0.8\n",
    "epochs = 1\n",
    "total_iterations = epochs * X.shape[0]\n",
    "\n",
    "# === Custom Weight Initialization Function ===\n",
    "def custom_weight_init(som, interval=0.3):\n",
    "    \"\"\"\n",
    "    Initialize SOM weights with values centered around zero in the given interval.\n",
    "    Uses a uniform distribution between [-interval, interval]\n",
    "    \"\"\"\n",
    "    som._weights = np.random.uniform(-interval, interval, \n",
    "                                    (som._weights.shape[0], \n",
    "                                    som._weights.shape[1], \n",
    "                                    som._weights.shape[2]))\n",
    "    return som\n",
    "\n",
    "# Initialize SOM\n",
    "som = MiniSom(x=num_rows, y=num_cols, input_len=X.shape[1],\n",
    "            sigma=sigma, learning_rate=learning_rate)\n",
    "\n",
    "# Use custom weight initialization centered on zero with interval [-0.3, 0.3]\n",
    "custom_weight_init(som, interval=0.3)\n",
    "\n",
    "# === Train SOM ===\n",
    "weights_evolution = []\n",
    "weights_evolution.append(som.get_weights().copy())\n",
    "\n",
    "try:\n",
    "    for iteration in range(total_iterations):\n",
    "        idx = np.random.randint(0, X.shape[0])\n",
    "        sample = X[idx]\n",
    "        \n",
    "        # Debug info for first few iterations\n",
    "        if iteration < 5:\n",
    "            print(f\"Iteration {iteration}, Sample shape: {sample.shape}, Sample type: {sample.dtype}\")\n",
    "            print(f\"Sample contains NaN: {np.isnan(sample).any()}\")\n",
    "        \n",
    "        # Ensure no NaN values in sample\n",
    "        if np.isnan(sample).any():\n",
    "            print(f\"NaN values found in sample at iteration {iteration}, replacing with zeros\")\n",
    "            sample = np.nan_to_num(sample)\n",
    "            \n",
    "        som.update(sample, som.winner(sample), iteration, total_iterations)\n",
    "\n",
    "        if iteration % (total_iterations // 10) == 0:\n",
    "            weights_evolution.append(som.get_weights().copy())\n",
    "            print(f\"Completed {iteration}/{total_iterations} iterations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # If we hit an error, try to print detailed info about the problematic sample\n",
    "    print(f\"Problem with sample at index {idx}:\")\n",
    "    print(f\"Sample: {sample}\")\n",
    "    print(f\"Sample dtype: {sample.dtype}\")\n",
    "    print(f\"Contains non-finite values: {np.isfinite(sample).all()}\")\n",
    "    raise\n",
    "\n",
    "# === Save the trained SOM model to a file ===\n",
    "joblib.dump(som, 'models2/som_model.joblib')\n",
    "print(\"SOM model trained and saved successfully!\")\n",
    "\n",
    "# === NEW CODE: AUTOMATICALLY GENERATE CLUSTER_OFFERS MAPPING ===\n",
    "# Function to parse offer strings from original data\n",
    "def parse_offers(offer_val):\n",
    "    if pd.isna(offer_val):\n",
    "        return []\n",
    "    try:\n",
    "        # Handle different data types from parquet vs CSV\n",
    "        if isinstance(offer_val, list):\n",
    "            offers = offer_val\n",
    "        elif isinstance(offer_val, str):\n",
    "            # Try parsing if it's a string representation of a list\n",
    "            if offer_val.startswith('[') and offer_val.endswith(']'):\n",
    "                try:\n",
    "                    offers = ast.literal_eval(offer_val)\n",
    "                except:\n",
    "                    offers = [offer_val]\n",
    "            else:\n",
    "                offers = [offer_val]\n",
    "        else:\n",
    "            offers = [offer_val]\n",
    "            \n",
    "        return offers if isinstance(offers, list) else [offers]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing offer: {offer_val}, Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Create a list of offers for each data point\n",
    "print(\"Generating cluster_offers mapping from SOM...\")\n",
    "original_offers_parsed = [parse_offers(offer) for offer in original_offers]\n",
    "\n",
    "# Find BMU for each data point\n",
    "bmu_indices = []\n",
    "for i, x in enumerate(X):\n",
    "    # Ensure no NaN values\n",
    "    if np.isnan(x).any():\n",
    "        x = np.nan_to_num(x)\n",
    "    winner = som.winner(x)\n",
    "    bmu_indices.append((winner[0], winner[1]))\n",
    "\n",
    "# Create a mapping from BMU to cluster ID\n",
    "cluster_mapping = {}\n",
    "for i, (row, col) in enumerate(bmu_indices):\n",
    "    cluster_id = row * num_cols + col\n",
    "    if cluster_id not in cluster_mapping:\n",
    "        cluster_mapping[cluster_id] = []\n",
    "    cluster_mapping[cluster_id].append(i)\n",
    "\n",
    "# Create a mapping from cluster ID to offers\n",
    "cluster_offers = {}\n",
    "for cluster_id, indices in cluster_mapping.items():\n",
    "    # Flatten list of lists and count occurrences\n",
    "    all_offers = [offer for idx in indices for offer in original_offers_parsed[idx]]\n",
    "    if not all_offers:\n",
    "        cluster_offers[cluster_id] = []\n",
    "        continue\n",
    "    \n",
    "    # Define offers to exclude globally\n",
    "    excluded_offers = {'F3000G200M', 'F3000G100M', 'F1200G50M'}\n",
    "    \n",
    "    offer_counts = Counter(all_offers)\n",
    "    \n",
    "    # Filter out excluded offers\n",
    "    filtered_offer_counts = {offer: count for offer, count in offer_counts.items() if offer not in excluded_offers}\n",
    "    \n",
    "    # Strategy: Take offers that appear in at least 20% of data points in the cluster\n",
    "    min_threshold = max(1, len(indices) * 0.2)\n",
    "    top_offers = [offer for offer, count in filtered_offer_counts.items() if count >= min_threshold]\n",
    "    \n",
    "    # If not enough, take next most common offers excluding the excluded ones\n",
    "    if len(top_offers) < 2:\n",
    "        # Get most common, but make sure we don't try to get more than we have\n",
    "        most_common_count = min(2, len(filtered_offer_counts))\n",
    "        top_offers = [offer for offer, _ in Counter(filtered_offer_counts).most_common(most_common_count)]\n",
    "\n",
    "    cluster_offers[cluster_id] = top_offers\n",
    "\n",
    "# Save the automatically generated cluster_offers mapping\n",
    "np.save('som_weights1.npy', som.get_weights())\n",
    "joblib.dump(cluster_offers, 'models2/cluster_offers.joblib')\n",
    "\n",
    "print(\"Cluster offers mapping generated and saved successfully!\")\n",
    "print(f\"Generated {len(cluster_offers)} cluster mappings.\")\n",
    "\n",
    "# Print a sample of the mappings\n",
    "print(\"\\nSample of cluster_offers mappings:\")\n",
    "sample_size = min(10, len(cluster_offers))\n",
    "sample_clusters = list(cluster_offers.keys())[:sample_size]\n",
    "for cluster in sample_clusters:\n",
    "    print(f\"Cluster {cluster}: {cluster_offers[cluster]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
